{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 규칙\n",
    "- 각 이름은 띄어쓰기 없이 작성하기. 예를 들어, \"영업 시간\"이 아니라 \"영업시간\"으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user09/beaver/data/shared_files/dataset/dataset_v4_general.xlsx\n",
      "/home/user09/beaver/data/shared_files/dataset/dataset_v4_general_102496_preprocessed.xlsx\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    # store=\"프랭크버거\"\n",
    "    output_path = \"/home/user09/beaver/data/db\"\n",
    "    data_path = \"\"\n",
    "    save_path = \"\"\n",
    "    embedding_model= \"nlpai-lab/KoE5\" # \"BAAI/bge-m3\"\n",
    "    retriever_k=5\n",
    "    retriever_bert_weight=0.7\n",
    "    version='4_general'\n",
    "    # info_type='STORE_INFO'   # STORE_INFO, TIME_INFO, MENU_INFO\n",
    "    store_num = 102496  # 102496, 102506, 103807, 104570, 104933\n",
    "    fill_nan = \"None\"  # 없는 정보의 경우에는 \"정보없음\"보다는 \"None\"이 나은 듯\n",
    "    # basic_info = [\"주차장\", \"씨씨티비\", \"영업시간\", \"예약가능여부\", \"전화번호\"]\n",
    "    seed=42\n",
    "\n",
    "\n",
    "CFG.data_path = f\"/home/user09/beaver/data/shared_files/dataset/dataset_v{CFG.version}.xlsx\"\n",
    "print(CFG.data_path)\n",
    "CFG.save_path = f\"/home/user09/beaver/data/shared_files/dataset/dataset_v{CFG.version}_{CFG.store_num}_preprocessed.xlsx\"\n",
    "print(CFG.save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STORE_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "히스피커피 서면본점\n",
      "                     ADDR          TEL   PAYMNT_MN_CD\n",
      "0  부산 부산진구 중앙대로 679-8 히스피  01098007047  현금, 신용카드, 포인트\n",
      "Index(['ADDR', 'TEL', 'PAYMNT_MN_CD'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "################ 전처리 ################\n",
    "from langchain.schema import Document\n",
    "\n",
    "# columns_to_use = ['STORE_NO', 'STORE_NM', 'ADDR', 'DETAIL_ADDR', 'TEL', 'GUID_CONTS', 'PAYMNT_MN_CD']\n",
    "columns_to_use = ['STORE_NO', 'STORE_NM', 'ADDR', 'DETAIL_ADDR', 'TEL', 'PAYMNT_MN_CD']  # 'GUID_CONTS'\n",
    "df_store = pd.read_excel(CFG.data_path, sheet_name='STORE_INFO', usecols=columns_to_use, dtype={\"TEL\": str})\n",
    "\n",
    "df_store = df_store[df_store['STORE_NO']==CFG.store_num]  # 해당 매장 정보만 가져옴\n",
    "store = df_store['STORE_NM'].unique()[0]\n",
    "print(store)\n",
    "df_store.drop(columns=['STORE_NO', 'STORE_NM'], inplace=True)\n",
    "\n",
    "# for col in df_store.columns:   # 매장 정보는 매장 당 한 개의 row만 존재하므로 제공해주지 않는 매장 정보 컬럼은 삭제\n",
    "#     if df_store[col].isnull().values.any():   \n",
    "#         df_store.drop(columns=[col], inplace=True)\n",
    "\n",
    "# df_2.drop(columns=['STORE_NO'], inplace=True)\n",
    "# print(df_2)\n",
    "\n",
    "# 1. 주소와 상세 주소 합치기\n",
    "df_store['ADDR'] = df_store['ADDR'] + ' ' + df_store['DETAIL_ADDR']\n",
    "df_store['ADDR'] = df_store['ADDR'].fillna(CFG.fill_nan)   # !!!!!!!!!!!!!!!\n",
    "df_store.drop(columns=['DETAIL_ADDR'], inplace=True)\n",
    "\n",
    "df_store.fillna(CFG.fill_nan, inplace=True)  # !!!!!!!!!!!!!!!\n",
    "\n",
    "# 2. 결제수단 매핑\n",
    "def map_payment_methods(payment_codes):\n",
    "    payment_mapping = {\n",
    "    'CA': '현금',\n",
    "    'CD': '신용카드',\n",
    "    'PO': '포인트',\n",
    "    'PC': '간편결제',\n",
    "    'GV': '모바일상품권',\n",
    "    'ET': '기타결제'\n",
    "    }\n",
    "    codes = payment_codes.split(', ')\n",
    "    mapped_names = [payment_mapping[code] for code in codes if code in payment_mapping]\n",
    "    return ', '.join(mapped_names)\n",
    "\n",
    "df_store['PAYMNT_MN_CD'] = df_store['PAYMNT_MN_CD'].apply(map_payment_methods)\n",
    "df_store_col = df_store.columns\n",
    "\n",
    "# 결과 출력\n",
    "print(df_store)\n",
    "print(df_store_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       위치         전화번호           결제수단\n",
      "0  부산 부산진구 중앙대로 679-8 히스피  01098007047  현금, 신용카드, 포인트\n"
     ]
    }
   ],
   "source": [
    "############## 컬럼명 변경 ##############\n",
    "store_column_mapping = {\n",
    "    'STORE_NO': '상점번호',\n",
    "    'STORE_NM': '상점명',\n",
    "    'ADDR': '위치',\n",
    "    'DETAIL_ADDR': '매장주소',\n",
    "    'TEL': '전화번호',\n",
    "    'X': '좌표 X',\n",
    "    'Y': '좌표 Y',\n",
    "    'GUID_TTL': '안내제목',  # !!!!!!!!!!!!!!!!!\n",
    "    'GUID_CONTS': '안내내용',\n",
    "    'PAYMNT_MN_CD': '결제수단',\n",
    "}\n",
    "\n",
    "for col in df_store_col:\n",
    "    df_store = df_store.rename(columns={col: store_column_mapping[col]})\n",
    "    \n",
    "print(df_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       contents\n",
      "0  '위치': 부산 부산진구 중앙대로 679-8 히스피\n",
      "1           '전화번호': 01098007047\n",
      "2         '결제수단': 현금, 신용카드, 포인트\n"
     ]
    }
   ],
   "source": [
    "############### 한 컬럼으로 통합 ###############\n",
    "def create_contents_rowwise(df):\n",
    "    contents_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        for col in df.columns:\n",
    "            contents_data.append(f\"'{col}': {row[col]}\")\n",
    "    new_df = pd.DataFrame({'contents': contents_data})\n",
    "    return new_df\n",
    "\n",
    "# 함수 호출\n",
    "result_df_store = create_contents_rowwise(df_store)\n",
    "print(result_df_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MENU_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CATEGORY_NM   PROD_NM  PROD_BASE_PRICE\n",
      "0        시그니처  히스피 크림라떼             4900\n",
      "1        시그니처  흑임자 크림라떼             4900\n",
      "2        시그니처   곡물 크림라떼             4900\n",
      "Index(['CATEGORY_NM', 'PROD_NM', 'PROD_BASE_PRICE'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "################ 전처리 ################\n",
    "columns_to_use = ['STORE_NO', 'PROD_NM', 'CATEGORY_NM', 'PROD_BASE_PRICE']\n",
    "df_menu = pd.read_excel(f'/home/user09/beaver/data/shared_files/dataset/dataset_v{CFG.version}.xlsx', sheet_name='MENU_INFO', usecols=columns_to_use)\n",
    "df_menu = df_menu[df_menu['STORE_NO']==CFG.store_num]\n",
    "df_menu.drop(columns=['STORE_NO'], inplace=True)\n",
    "\n",
    "df_menu_col = df_menu.columns\n",
    "\n",
    "print(df_menu[:3])\n",
    "print(df_menu_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   카테고리       메뉴명     가격\n",
      "0  시그니처  히스피 크림라떼  4900원\n",
      "1  시그니처  흑임자 크림라떼  4900원\n",
      "2  시그니처   곡물 크림라떼  4900원\n"
     ]
    }
   ],
   "source": [
    "############## 컬럼명 변경 ##############\n",
    "menu_column_mapping = {\n",
    "    # 'PROD_NO': '상품 번호',\n",
    "    'PROD_NM': '메뉴명',\n",
    "    'CATEGORY_NM': '카테고리',\n",
    "    'PROD_BASE_PRICE': '가격'\n",
    "}\n",
    "\n",
    "for col in df_menu_col:\n",
    "    df_menu = df_menu.rename(columns={col: menu_column_mapping[col]})\n",
    "\n",
    "df_menu['가격'] = df_menu['가격'].astype(str) + '원'\n",
    "    \n",
    "print(df_menu[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     contents\n",
      "0  '카테고리': 시그니처, '메뉴명': 히스피 크림라떼, '가격': 4900원\n",
      "1  '카테고리': 시그니처, '메뉴명': 흑임자 크림라떼, '가격': 4900원\n",
      "2   '카테고리': 시그니처, '메뉴명': 곡물 크림라떼, '가격': 4900원\n"
     ]
    }
   ],
   "source": [
    "############### 한 컬럼으로 통합 ###############\n",
    "def create_contents_rowwise(df):\n",
    "    contents_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        row_content = \", \".join([f\"'{col}': {row[col]}\" for col in df.columns])  # 각 행의 컬럼명:값을 \", \"로 결합하여 하나의 문자열 생성\n",
    "        contents_data.append(row_content)\n",
    "    \n",
    "    new_df = pd.DataFrame({'contents': contents_data})  # contents 열을 포함하는 새로운 데이터프레임 생성\n",
    "    return new_df\n",
    "\n",
    "# 함수 호출\n",
    "result_df_menu = create_contents_rowwise(df_menu)\n",
    "print(result_df_menu[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIME_INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   STORE_NO  WKDY SALE_BGN_TM SALE_END_TM STORE_REST_BGN_TM STORE_REST_END_TM\n",
      "0    102496  None        None        None              None              None\n"
     ]
    }
   ],
   "source": [
    "### 전처리 ###\n",
    "columns_to_use = ['STORE_NO', 'WKDY', 'SALE_BGN_TM', 'SALE_END_TM', 'STORE_REST_BGN_TM', 'STORE_REST_END_TM']\n",
    "df_time = pd.read_excel(f'/home/user09/beaver/data/shared_files/dataset/dataset_v{CFG.version}.xlsx', sheet_name='TIME_INFO', usecols=columns_to_use)\n",
    "df_time = df_time[df_time['STORE_NO']==CFG.store_num]\n",
    "\n",
    "# 1. 요일 매핑\n",
    "day_mapping = {\n",
    "    2: '월요일',\n",
    "    3: '화요일',\n",
    "    4: '수요일',\n",
    "    5: '목요일',\n",
    "    6: '금요일',\n",
    "    7: '토요일',\n",
    "    1: '일요일'\n",
    "}\n",
    "df_time['WKDY'] = df_time['WKDY'].map(day_mapping)\n",
    "df_time.fillna(CFG.fill_nan, inplace=True)\n",
    "\n",
    "def convert_time_format(hhmmss):\n",
    "    if pd.isna(hhmmss) or hhmmss in [None, 'None']:\n",
    "        return CFG.fill_nan\n",
    "    hhmmss = str(int(hhmmss)).zfill(6)\n",
    "    return hhmmss[:2] + ':' + hhmmss[2:4]\n",
    "\n",
    "def handle_missing_values(df, columns):\n",
    "    for col in columns:\n",
    "        if df[col].isna().all():  # 해당 매장의 해당 컬럼의 모든 값이 없으면, 정보가 없는 것으로 간주하여 None값으로 채움\n",
    "            df[col] = CFG.fill_nan \n",
    "        else:                     # 해당 매장의 해당 컬럼의 일부 값이 있으면, 정보가 없는 것이 아니라 해당 요일은 브레이크 타임이 없는 것으로 간주하여 \"없음\"으로 채움\n",
    "            df[col] = df[col].fillna(\"없음\")  # NaN인 부분만 \"없음\"으로 채움\n",
    "    return df\n",
    "\n",
    "# 시간 변환 함수 적용\n",
    "df_time['SALE_BGN_TM'] = df_time['SALE_BGN_TM'].apply(convert_time_format)\n",
    "df_time['SALE_END_TM'] = df_time['SALE_END_TM'].apply(convert_time_format)\n",
    "df_time['STORE_REST_BGN_TM'] = df_time['STORE_REST_BGN_TM'].apply(convert_time_format)\n",
    "df_time['STORE_REST_END_TM'] = df_time['STORE_REST_END_TM'].apply(convert_time_format)\n",
    "\n",
    "# 빈값 처리 함수 적용\n",
    "columns_to_check = ['SALE_BGN_TM', 'SALE_END_TM', 'STORE_REST_BGN_TM', 'STORE_REST_END_TM']\n",
    "df_time = handle_missing_values(df_time, columns_to_check)\n",
    "\n",
    "print(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 분리\n",
    "# def process_business_hours(df):\n",
    "#     grouped = df.groupby(\n",
    "#         [\"SALE_BGN_TM\", \"SALE_END_TM\"]\n",
    "#     )[\"WKDY\"].apply(list).reset_index()\n",
    "    \n",
    "#     combined_data = {}\n",
    "#     for _, row in grouped.iterrows():\n",
    "#         if row[\"SALE_BGN_TM\"] == CFG.fill_nan and row[\"SALE_END_TM\"] == CFG.fill_nan:\n",
    "#             continue\n",
    "#         else:\n",
    "#             # Flatten WKDY if nested lists are present\n",
    "#             weekdays = \"&\".join([day for sublist in row[\"WKDY\"] for day in sublist] if isinstance(row[\"WKDY\"][0], list) else row[\"WKDY\"])\n",
    "#             # Add 영업시간 to combined_data\n",
    "#             combined_data[f\"'{weekdays}_영업시간'\"] = f\"{row['SALE_BGN_TM']}-{row['SALE_END_TM']}\"\n",
    "\n",
    "#     if not combined_data:\n",
    "#         combined_data = CFG.fill_nan\n",
    "\n",
    "#     # Format combined_data\n",
    "#     formatted_combined_data = \", \".join([f\"{key}: {value}\" for key, value in combined_data.items()])\n",
    "\n",
    "#     result = {\"contents\": formatted_combined_data}\n",
    "#     new_df = pd.DataFrame([result])\n",
    "#     return new_df\n",
    "\n",
    "# # Apply the function\n",
    "# business_hours_df = process_business_hours(df_time)\n",
    "# print(business_hours_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      contents\n",
      "0  '영업시간':None\n"
     ]
    }
   ],
   "source": [
    "def process_business_hours(df):\n",
    "    grouped = df.groupby(\n",
    "        [\"SALE_BGN_TM\", \"SALE_END_TM\"]\n",
    "    )[\"WKDY\"].apply(list).reset_index()\n",
    "    \n",
    "    combined_data = {}\n",
    "    for _, row in grouped.iterrows():\n",
    "        if row[\"SALE_BGN_TM\"] == CFG.fill_nan and row[\"SALE_END_TM\"] == CFG.fill_nan:\n",
    "            continue\n",
    "        else:\n",
    "            # Flatten WKDY if nested lists are present\n",
    "            weekdays = \"&\".join([day for sublist in row[\"WKDY\"] for day in sublist] if isinstance(row[\"WKDY\"][0], list) else row[\"WKDY\"])\n",
    "            # Add 영업시간 to combined_data\n",
    "            combined_data[f\"'{weekdays}_영업시간'\"] = f\"{row['SALE_BGN_TM']}-{row['SALE_END_TM']}\"\n",
    "\n",
    "    if not combined_data:\n",
    "        combined_data = {}  # 빈 딕셔너리로 설정\n",
    "\n",
    "    # Format combined_data\n",
    "    formatted_combined_data = \", \".join([f\"{key}: {value}\" for key, value in combined_data.items()])\n",
    "\n",
    "    if formatted_combined_data == \"\":\n",
    "        formatted_combined_data = f\"'영업시간':{CFG.fill_nan}\"\n",
    "    result = {\"contents\": formatted_combined_data}\n",
    "    new_df = pd.DataFrame([result])\n",
    "    return new_df\n",
    "\n",
    "# Apply the function\n",
    "business_hours_df = process_business_hours(df_time)\n",
    "print(business_hours_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 분리\n",
    "# def process_break_time(df):\n",
    "#     grouped = df.groupby(\n",
    "#         [\"STORE_REST_BGN_TM\", \"STORE_REST_END_TM\"]\n",
    "#     )[\"WKDY\"].apply(list).reset_index()\n",
    "    \n",
    "#     combined_data = {}\n",
    "#     for _, row in grouped.iterrows():\n",
    "#         if row[\"STORE_REST_BGN_TM\"] == \"없음\" or row[\"STORE_REST_BGN_TM\"] == CFG.fill_nan:\n",
    "#             break_time = \"브레이크타임없음\"\n",
    "#         else:\n",
    "#             break_time = f\"{row['STORE_REST_BGN_TM']}~{row['STORE_REST_END_TM']}\"\n",
    "        \n",
    "#         # Flatten WKDY if nested lists are present\n",
    "#         weekdays = \"&\".join([day for sublist in row[\"WKDY\"] for day in sublist] if isinstance(row[\"WKDY\"][0], list) else row[\"WKDY\"])\n",
    "#         # Add 브레이크타임 to combined_data\n",
    "#         combined_data[f\"'{weekdays}_브레이크타임'\"] = break_time\n",
    "\n",
    "#     if not combined_data:\n",
    "#         combined_data = CFG.fill_nan\n",
    "\n",
    "#     # Format combined_data\n",
    "#     formatted_combined_data = \", \".join([f\"{key}: {value}\" for key, value in combined_data.items()])\n",
    "\n",
    "#     result = {\"contents\": formatted_combined_data}\n",
    "#     new_df = pd.DataFrame([result])\n",
    "#     return new_df\n",
    "\n",
    "# # Apply the function\n",
    "# break_time_df = process_break_time(df_time)\n",
    "# print(break_time_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         contents\n",
      "0  '브레이크타임': None\n"
     ]
    }
   ],
   "source": [
    "def process_break_time_with_none(df):\n",
    "    grouped = df.groupby(\n",
    "        [\"STORE_REST_BGN_TM\", \"STORE_REST_END_TM\"]\n",
    "    )[\"WKDY\"].apply(list).reset_index()\n",
    "    \n",
    "    combined_data = {}\n",
    "    for _, row in grouped.iterrows():\n",
    "        if row[\"STORE_REST_BGN_TM\"] == \"없음\" or row[\"STORE_REST_BGN_TM\"] == CFG.fill_nan:\n",
    "            break_time = \"브레이크타임없음\"\n",
    "        else:\n",
    "            break_time = f\"{row['STORE_REST_BGN_TM']}~{row['STORE_REST_END_TM']}\"\n",
    "\n",
    "        # Handle None in WKDY\n",
    "        if row[\"WKDY\"] is None or all(day is None for day in row[\"WKDY\"]):\n",
    "            weekdays = \"None\"\n",
    "        else:\n",
    "            # Flatten WKDY if nested lists are present\n",
    "            weekdays = \"&\".join([day for sublist in row[\"WKDY\"] for day in sublist] if isinstance(row[\"WKDY\"][0], list) else row[\"WKDY\"])\n",
    "        \n",
    "        # Add 브레이크타임 to combined_data\n",
    "        if weekdays == \"None\":\n",
    "            combined_data[\"'브레이크타임'\"] = None\n",
    "        else:\n",
    "            combined_data[f\"'{weekdays}_브레이크타임'\"] = break_time\n",
    "\n",
    "    if not combined_data:\n",
    "        combined_data = {\"브레이크타임\": \"None\"}\n",
    "\n",
    "    # Format combined_data\n",
    "    formatted_combined_data = \", \".join([f\"{key}: {value}\" for key, value in combined_data.items()])\n",
    "\n",
    "    result = {\"contents\": formatted_combined_data}\n",
    "    new_df = pd.DataFrame([result])\n",
    "    return new_df\n",
    "\n",
    "# Apply the function\n",
    "break_time_df = process_break_time_with_none(df_time)\n",
    "print(break_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         contents\n",
      "0     '영업시간':None\n",
      "1  '브레이크타임': None\n"
     ]
    }
   ],
   "source": [
    "# 합치기\n",
    "result_df_time = pd.concat([business_hours_df, break_time_df], ignore_index=True)\n",
    "print(result_df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 전체 합친거\n",
    "# def group_by_schedule_to_single_row_fixed(df):\n",
    "#     grouped = df.groupby(\n",
    "#         [\"SALE_BGN_TM\", \"SALE_END_TM\", \"STORE_REST_BGN_TM\", \"STORE_REST_END_TM\"]\n",
    "#     )[\"WKDY\"].apply(list).reset_index()\n",
    "    \n",
    "#     combined_data = {}\n",
    "#     for _, row in grouped.iterrows():\n",
    "#         if (\n",
    "#             row[\"SALE_BGN_TM\"] == CFG.fill_nan and \n",
    "#             row[\"SALE_END_TM\"] == CFG.fill_nan and \n",
    "#             row[\"STORE_REST_BGN_TM\"] == CFG.fill_nan and \n",
    "#             row[\"STORE_REST_END_TM\"] == CFG.fill_nan\n",
    "#         ):\n",
    "#             continue\n",
    "#         else:\n",
    "#             if row[\"STORE_REST_BGN_TM\"] == \"없음\" or row[\"STORE_REST_BGN_TM\"] == CFG.fill_nan:\n",
    "#                 break_time = \"브레이크타임없음\"\n",
    "#             else:\n",
    "#                 break_time = f\"{row['STORE_REST_BGN_TM']}~{row['STORE_REST_END_TM']}\"\n",
    "            \n",
    "#             # Flatten WKDY if nested lists are present\n",
    "#             weekdays = \"&\".join([day for sublist in row[\"WKDY\"] for day in sublist] if isinstance(row[\"WKDY\"][0], list) else row[\"WKDY\"])\n",
    "            \n",
    "#             # Add formatted data to combined_data\n",
    "#             combined_data[f\"'{weekdays}_영업시간'\"] = f\"{row['SALE_BGN_TM']}-{row['SALE_END_TM']}\"\n",
    "#             combined_data[f\"'{weekdays}_브레이크타임'\"] = break_time\n",
    "\n",
    "#     if not combined_data:\n",
    "#         combined_data = CFG.fill_nan\n",
    "\n",
    "#     formatted_combined_data = (\n",
    "#         \"{\" +\n",
    "#         \", \".join([f\"{key}: {value}\" for key, value in combined_data.items()]) +\n",
    "#         \"}\"\n",
    "#     )\n",
    "\n",
    "#     result = {\"contents\": formatted_combined_data}\n",
    "#     new_df = pd.DataFrame([result])\n",
    "#     return new_df\n",
    "\n",
    "# # Apply the function\n",
    "# result_df_time = group_by_schedule_to_single_row_fixed(df_time)\n",
    "\n",
    "# print(result_df_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 휴무일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  HOLIDAY_TYPE_CD\n",
      "0            None\n"
     ]
    }
   ],
   "source": [
    "columns_to_use = ['STORE_NO', 'HOLIDAY_TYPE_CD']\n",
    "df_holiday = pd.read_excel(f'/home/user09/beaver/data/shared_files/dataset/dataset_v{CFG.version}.xlsx', sheet_name='HOLIDAY_INFO', usecols=columns_to_use)\n",
    "df_holiday = df_holiday[df_holiday['STORE_NO']==CFG.store_num]   # 해당 매장 정보만 가져옴\n",
    "df_holiday.fillna(CFG.fill_nan, inplace=True)\n",
    "df_holiday.drop(columns=['STORE_NO'], inplace=True)\n",
    "df_holiday_col = df_holiday.columns\n",
    "print(df_holiday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    휴무일\n",
      "0  None\n"
     ]
    }
   ],
   "source": [
    "############## 컬럼명 변경 ##############\n",
    "day_column_mapping = {\n",
    "    # 'PROD_NO': '상품 번호',\n",
    "    'HOLIDAY_TYPE_CD': '휴무일',\n",
    "}\n",
    "\n",
    "for col in df_holiday_col:\n",
    "    df_holiday = df_holiday.rename(columns={col: day_column_mapping[col]})\n",
    "    \n",
    "print(df_holiday[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      contents\n",
      "0  '휴무일': None\n"
     ]
    }
   ],
   "source": [
    "############### 한 컬럼으로 통합 ###############\n",
    "def create_contents_rowwise(df):\n",
    "    contents_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        row_content = \", \".join([f\"'{col}': {row[col]}\" for col in df.columns])  # 각 행의 컬럼명:값을 \", \"로 결합하여 하나의 문자열 생성\n",
    "        contents_data.append(row_content)\n",
    "    \n",
    "    new_df = pd.DataFrame({'contents': contents_data})  # contents 열을 포함하는 새로운 데이터프레임 생성\n",
    "    return new_df\n",
    "\n",
    "# 함수 호출\n",
    "result_df_holiday = create_contents_rowwise(df_holiday)\n",
    "print(result_df_holiday[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4개 통합 & 피클파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([result_df_time, result_df_store, result_df_menu, result_df_holiday], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         contents\n",
      "0                                     '영업시간':None\n",
      "1                                  '브레이크타임': None\n",
      "2                    '위치': 부산 부산진구 중앙대로 679-8 히스피\n",
      "3                             '전화번호': 01098007047\n",
      "4                           '결제수단': 현금, 신용카드, 포인트\n",
      "..                                            ...\n",
      "256           '카테고리': 시즌, '메뉴명': 옥스피, '가격': 4400원\n",
      "257        '카테고리': 시즌, '메뉴명': 달밤크림라떼, '가격': 5500원\n",
      "258       '카테고리': 시즌, '메뉴명': 달밤치즈프라페, '가격': 5900원\n",
      "259  '카테고리': 시즌, '메뉴명': 가을 디저트 선물세트, '가격': 16900원\n",
      "260                                   '휴무일': None\n",
      "\n",
      "[261 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 정보 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \":\"를 기준으로 분리하여 앞의 키워드를 추출한 후, 존재 여부 확인\n",
    "# existing_info = [content.split(\":\")[0].strip() for content in df[\"contents\"]]\n",
    "# missing_info = [info for info in CFG.basic_info if info not in existing_info]\n",
    "# for info in missing_info:\n",
    "#     df = pd.concat([df, pd.DataFrame({\"contents\": [f\"{info}: 정보없음\"]})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         contents\n",
      "0                                     '영업시간':None\n",
      "1                                  '브레이크타임': None\n",
      "2                    '위치': 부산 부산진구 중앙대로 679-8 히스피\n",
      "3                             '전화번호': 01098007047\n",
      "4                           '결제수단': 현금, 신용카드, 포인트\n",
      "..                                            ...\n",
      "256           '카테고리': 시즌, '메뉴명': 옥스피, '가격': 4400원\n",
      "257        '카테고리': 시즌, '메뉴명': 달밤크림라떼, '가격': 5500원\n",
      "258       '카테고리': 시즌, '메뉴명': 달밤치즈프라페, '가격': 5900원\n",
      "259  '카테고리': 시즌, '메뉴명': 가을 디저트 선물세트, '가격': 16900원\n",
      "260                                   '휴무일': None\n",
      "\n",
      "[261 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# 결과 저장 및 출력\n",
    "df.to_excel(CFG.save_path, index=False)   # 전처리한 엑셀 파일 저장\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user09/venv/beaver/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "#### 엑셀파일 DB화(pickle파일로 변환) ##### Extracting information from the \"Details\" column to create documents\n",
    "docs = []\n",
    "for _, row in df.iterrows():\n",
    "    details = row['contents']\n",
    "    # metadata = {}\n",
    "    \n",
    "    # # Parse key-value pairs in the \"Details\" column\n",
    "    # for detail in details.split('\\n'):\n",
    "    #     if ':' in detail:\n",
    "    #         key, value = detail.split(':', 1)\n",
    "            # metadata[key.strip()] = value.strip()\n",
    "    \n",
    "    docs.append(Document(page_content=details))#, metadata=metadata))\n",
    "\n",
    "# Embeddings and vector store setup\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=CFG.embedding_model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(docs, hf)\n",
    "\n",
    "# Save the FAISS vector store and documents as pickle\n",
    "db.save_local(f\"{CFG.output_path}/{CFG.store_num}_faiss{CFG.version}\")\n",
    "\n",
    "with open(f\"{CFG.output_path}/{CFG.store_num}_docs{CFG.version}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(docs, f)\n",
    "\n",
    "# Load the FAISS vector store\n",
    "db = FAISS.load_local(f\"/home/user09/beaver/data/db/{CFG.store_num}_faiss{CFG.version}\", hf, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Create retrievers\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": CFG.retriever_k})\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = CFG.retriever_k\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[retriever, bm25_retriever],\n",
    "    weights=[CFG.retriever_bert_weight, 1 - CFG.retriever_bert_weight]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beaver3.10",
   "language": "python",
   "name": "beaver"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
